{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_activity = 'all'\n",
    "import pandas as pd\n",
    "train = pd.read_csv('https://classes.soe.ucsc.edu/cmps242/Spring18/hw/hw3/train.csv',encoding='Latin-1')\n",
    "test = pd.read_csv('https://classes.soe.ucsc.edu/cmps242/Spring18/hw/hw3/test.csv',encoding='Latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filter_sms = [sms for sms in train.sms.values]\n",
    "word_token = [word_tokenize(sms) for sms in filter_sms]\n",
    "filter_words = []\n",
    "for word in word_token:\n",
    "    filter_words.append([w for w in word if not w in stop_words])\n",
    "filter_sms = []\n",
    "for word in filter_words:\n",
    "    filter_sms.append(' '.join(word))\n",
    "vectorizer = CountVectorizer()\n",
    "train_token = vectorizer.fit_transform(filter_sms)\n",
    "\n",
    "# tfidf transform\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf = True)\n",
    "tfidf_train_token = transformer.fit_transform(train_token)\n",
    "tfidf_train_token = tfidf_train_token.toarray()\n",
    "colname = vectorizer.get_feature_names()\n",
    "df_token = pd.DataFrame(data=tfidf_train_token,columns=colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizer\n",
    "from sklearn import preprocessing\n",
    "standardScaler = preprocessing.StandardScaler(with_mean=False)\n",
    "standard_df_token = standardScaler.fit_transform(df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1500)\n",
    "df_token_pca=pca.fit_transform(standard_df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias term\n",
    "import numpy as np\n",
    "train_x = np.hstack((np.ones((3000,1)),df_token_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "np.random.seed(0) # set the seed to 0\n",
    "w_init = np.random.rand(1501,1)\n",
    "train_y=pd.get_dummies(train.label.values)['spam'].values\n",
    "train_y=train_y.astype('float').reshape(len(train_y),1)\n",
    "\n",
    "# concatenate x and y\n",
    "trainset = np.hstack((train_x,train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "cost function:$$L = -y\\log(\\sigma(XW))-(1-y)\\log(1-\\sigma(XW)) + \\lambda||W||^2$$\n",
    "gradient:  $$diag(\\sigma(WX) - y)X + \\lambda |W|$$\n",
    "update: $$w = w - \\eta \\times gradient$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood function/logit loss\n",
    "def l_loss(y_,y):\n",
    "    if y==1:\n",
    "        loss = -np.log(y_)\n",
    "    elif y==0:\n",
    "        loss = -np.log(1-y_)\n",
    "    return loss\n",
    "l_loss = np.vectorize(l_loss)\n",
    "\n",
    "def logitloss(y_,y):\n",
    "    loss = l_loss(y_,y)\n",
    "    where_inf = np.isinf(loss)\n",
    "    loss[where_inf] = 150\n",
    "    return loss\n",
    "\n",
    "def sigmoid(w,x):\n",
    "    return np.divide(np.exp(np.dot(x,w)),(np.add(1,np.exp(np.dot(x,w)))))\n",
    "\n",
    "def gradient(y,x,w,lmd):\n",
    "    #loss = logitloss(sigmoid(w,x),y).reshape(y.shape[0],)\n",
    "    reg = lmd*w\n",
    "    reg[0] = 0 # not regularize the bias term\n",
    "    return np.add(reg,np.dot(np.diag(np.subtract(sigmoid(w,x).reshape(y.shape[0],),y)),x)).mean(axis=0).reshape((1501,1))\n",
    "    \n",
    "\n",
    "def gradientDescent(y,w,x,eta,lmd):\n",
    "    # eta: learning rate\n",
    "    # lmb: regularization term\n",
    "    for i in range(1000):\n",
    "        lossPre = logitloss(sigmoid(w,x),y).sum()\n",
    "        w = np.subtract(w,np.multiply(eta,gradient(y,x,w,lmd)))\n",
    "        lossPost = logitloss(sigmoid(w,x),y).sum()\n",
    "        if (lossPre-lossPost)/lossPre<0.05:\n",
    "            break\n",
    "    return w\n",
    "        \n",
    "\n",
    "# vectorize functions\n",
    "logitloss = np.vectorize(logitloss)\n",
    "\n",
    "#w_opt = gradientDescent(train_y, w_init, train_x, 0.01, 0.1)\n",
    "#w_opt.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianglongtan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal loss:  26.54992165939248\n",
      "optimal loss:  41.70435069424559\n",
      "optimal loss:  44.001390289468105\n",
      "optimal loss:  32.027334575676846\n",
      "optimal loss:  42.37253557698297\n",
      "optimal loss:  31.734910491201124\n",
      "optimal loss:  43.60933282148682\n",
      "optimal loss:  41.77293885605393\n",
      "optimal loss:  48.23768532912316\n",
      "optimal loss:  0.8727859904801794\n",
      "[[16.58770666]\n",
      " [10.94745867]\n",
      " [12.26798859]\n",
      " [13.03775255]\n",
      " [14.07733039]\n",
      " [12.99261079]\n",
      " [12.62274288]\n",
      " [13.805076  ]\n",
      " [12.35283628]\n",
      " [10.73982546]]\n",
      "optimal loss:  32.4687241372396\n",
      "optimal loss:  40.53195016988851\n",
      "optimal loss:  34.996822495319634\n",
      "optimal loss:  30.709264470630327\n",
      "optimal loss:  23.69551813039955\n",
      "optimal loss:  32.5790295022225\n",
      "optimal loss:  38.87831676770543\n",
      "optimal loss:  26.541600627745737\n",
      "optimal loss:  27.166106630736998\n",
      "optimal loss:  1.016233161884004\n",
      "[[16.58770666 18.23250276]\n",
      " [10.94745867 13.13485958]\n",
      " [12.26798859 13.77732841]\n",
      " [13.03775255 13.66347995]\n",
      " [14.07733039 15.26415215]\n",
      " [12.99261079 12.94292481]\n",
      " [12.62274288 14.18574218]\n",
      " [13.805076   12.35864492]\n",
      " [12.35283628 15.06827078]\n",
      " [10.73982546  9.93575485]]\n",
      "optimal loss:  35.52242385636663\n",
      "optimal loss:  39.44706238537497\n",
      "optimal loss:  42.655734135823096\n",
      "optimal loss:  87.60639273766148\n",
      "optimal loss:  28.647585494700586\n",
      "optimal loss:  35.93414291717473\n",
      "optimal loss:  46.962346581647516\n",
      "optimal loss:  39.480199462691985\n",
      "optimal loss:  24.86141513258899\n",
      "optimal loss:  1.0520662375138554\n",
      "[[16.58770666 18.23250276 16.25709493]\n",
      " [10.94745867 13.13485958 14.48018757]\n",
      " [12.26798859 13.77732841 13.32021978]\n",
      " [13.03775255 13.66347995 14.21750778]\n",
      " [14.07733039 15.26415215 15.00335284]\n",
      " [12.99261079 12.94292481 12.48821207]\n",
      " [12.62274288 14.18574218 12.13124712]\n",
      " [13.805076   12.35864492 12.44334725]\n",
      " [12.35283628 15.06827078 14.16603624]\n",
      " [10.73982546  9.93575485 10.35932983]]\n",
      "optimal loss:  21.40632875174181\n",
      "optimal loss:  21.062971113681968\n",
      "optimal loss:  40.835019091960326\n",
      "optimal loss:  22.908924058013493\n",
      "optimal loss:  30.317884774247982\n",
      "optimal loss:  28.18811420733569\n",
      "optimal loss:  47.814682968993125\n",
      "optimal loss:  52.43536427032874\n",
      "optimal loss:  23.514695011319475\n",
      "optimal loss:  0.9982487580660583\n",
      "[[16.58770666 18.23250276 16.25709493 16.09878902]\n",
      " [10.94745867 13.13485958 14.48018757 13.52964475]\n",
      " [12.26798859 13.77732841 13.32021978 13.35913264]\n",
      " [13.03775255 13.66347995 14.21750778 13.58290567]\n",
      " [14.07733039 15.26415215 15.00335284 15.94158238]\n",
      " [12.99261079 12.94292481 12.48821207 12.41087086]\n",
      " [12.62274288 14.18574218 12.13124712 12.20908306]\n",
      " [13.805076   12.35864492 12.44334725 13.69170502]\n",
      " [12.35283628 15.06827078 14.16603624 14.9358052 ]\n",
      " [10.73982546  9.93575485 10.35932983  9.18005051]]\n",
      "optimal loss:  34.68722814460189\n",
      "optimal loss:  27.355282037009832\n",
      "optimal loss:  36.692305589590056\n",
      "optimal loss:  39.863287619530595\n",
      "optimal loss:  51.49233493014436\n",
      "optimal loss:  33.02396002718599\n",
      "optimal loss:  40.361959160274466\n",
      "optimal loss:  30.729816662467222\n",
      "optimal loss:  26.69464328463292\n",
      "optimal loss:  0.9670668500434326\n",
      "[[16.58770666 18.23250276 16.25709493 16.09878902 15.13980463]\n",
      " [10.94745867 13.13485958 14.48018757 13.52964475 11.05106384]\n",
      " [12.26798859 13.77732841 13.32021978 13.35913264 10.71953243]\n",
      " [13.03775255 13.66347995 14.21750778 13.58290567 10.51582156]\n",
      " [14.07733039 15.26415215 15.00335284 15.94158238 12.76041134]\n",
      " [12.99261079 12.94292481 12.48821207 12.41087086 12.29918981]\n",
      " [12.62274288 14.18574218 12.13124712 12.20908306 13.06475395]\n",
      " [13.805076   12.35864492 12.44334725 13.69170502 10.04622221]\n",
      " [12.35283628 15.06827078 14.16603624 14.9358052  14.22372242]\n",
      " [10.73982546  9.93575485 10.35932983  9.18005051  9.3039275 ]]\n",
      "optimal loss:  30.31946556032223\n",
      "optimal loss:  49.663073249188685\n",
      "optimal loss:  40.61527647866995\n",
      "optimal loss:  31.55262511623003\n",
      "optimal loss:  45.517981476177326\n",
      "optimal loss:  41.6407056230185\n",
      "optimal loss:  39.030901430342205\n",
      "optimal loss:  39.83190658802065\n",
      "optimal loss:  41.98060489246918\n",
      "optimal loss:  0.8877834896970509\n",
      "[[16.58770666 18.23250276 16.25709493 16.09878902 15.13980463 12.74584858]\n",
      " [10.94745867 13.13485958 14.48018757 13.52964475 11.05106384 10.16923366]\n",
      " [12.26798859 13.77732841 13.32021978 13.35913264 10.71953243  8.71562933]\n",
      " [13.03775255 13.66347995 14.21750778 13.58290567 10.51582156  9.2416294 ]\n",
      " [14.07733039 15.26415215 15.00335284 15.94158238 12.76041134 10.57152195]\n",
      " [12.99261079 12.94292481 12.48821207 12.41087086 12.29918981  8.90684081]\n",
      " [12.62274288 14.18574218 12.13124712 12.20908306 13.06475395  8.41395785]\n",
      " [13.805076   12.35864492 12.44334725 13.69170502 10.04622221  8.79548712]\n",
      " [12.35283628 15.06827078 14.16603624 14.9358052  14.22372242  9.54521175]\n",
      " [10.73982546  9.93575485 10.35932983  9.18005051  9.3039275   8.55614954]]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "lambda_candi = [0,0.01,0.1,1,10,100]\n",
    "error_sum = []\n",
    "i = 1\n",
    "for lmd in lambda_candi:\n",
    "    error = []\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        w_init = np.random.rand(1501,1)\n",
    "        X_train, y_train = train_x[train_index], train_y[train_index]\n",
    "        X_test, y_test = train_x[test_index], train_y[test_index]\n",
    "        w_opt = gradientDescent(y_train,w_init,X_train,0.01,lmd)\n",
    "        loss = logitloss(sigmoid(w_opt,X_test), y_test).mean(axis=0)\n",
    "        error.append(loss)\n",
    "    error = np.array(error).reshape(10,1)\n",
    "    if i==1:\n",
    "        error_sum = error\n",
    "    else:\n",
    "        error_sum = np.hstack((error_sum,error))\n",
    "    print(error_sum)\n",
    "    i += 1\n",
    "print('Done!')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n",
      " 0.00      12.943133\n",
      "0.01      13.856366\n",
      "0.10      13.486654\n",
      "1.00      13.493957\n",
      "10.00     11.912445\n",
      "100.00     9.566151\n",
      "Name: mean, dtype: float64\n",
      "standart deviation:\n",
      " 0.00      1.668927\n",
      "0.01      2.153096\n",
      "0.10      1.689474\n",
      "1.00      2.009842\n",
      "10.00     1.896108\n",
      "100.00    1.320369\n",
      "Name: std, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>100.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.943133</td>\n",
       "      <td>13.856366</td>\n",
       "      <td>13.486654</td>\n",
       "      <td>13.493957</td>\n",
       "      <td>11.912445</td>\n",
       "      <td>9.566151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.668927</td>\n",
       "      <td>2.153096</td>\n",
       "      <td>1.689474</td>\n",
       "      <td>2.009842</td>\n",
       "      <td>1.896108</td>\n",
       "      <td>1.320369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.739825</td>\n",
       "      <td>9.935755</td>\n",
       "      <td>10.359330</td>\n",
       "      <td>9.180051</td>\n",
       "      <td>9.303928</td>\n",
       "      <td>8.413958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.289201</td>\n",
       "      <td>12.990909</td>\n",
       "      <td>12.454563</td>\n",
       "      <td>12.647936</td>\n",
       "      <td>10.566749</td>\n",
       "      <td>8.735594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.807677</td>\n",
       "      <td>13.720404</td>\n",
       "      <td>13.743128</td>\n",
       "      <td>13.556275</td>\n",
       "      <td>11.675127</td>\n",
       "      <td>9.074235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.613245</td>\n",
       "      <td>14.847639</td>\n",
       "      <td>14.414518</td>\n",
       "      <td>14.624780</td>\n",
       "      <td>12.988668</td>\n",
       "      <td>10.013228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.587707</td>\n",
       "      <td>18.232503</td>\n",
       "      <td>16.257095</td>\n",
       "      <td>16.098789</td>\n",
       "      <td>15.139805</td>\n",
       "      <td>12.745849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.00       0.01       0.10       1.00       10.00      100.00\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean   12.943133  13.856366  13.486654  13.493957  11.912445   9.566151\n",
       "std     1.668927   2.153096   1.689474   2.009842   1.896108   1.320369\n",
       "min    10.739825   9.935755  10.359330   9.180051   9.303928   8.413958\n",
       "25%    12.289201  12.990909  12.454563  12.647936  10.566749   8.735594\n",
       "50%    12.807677  13.720404  13.743128  13.556275  11.675127   9.074235\n",
       "75%    13.613245  14.847639  14.414518  14.624780  12.988668  10.013228\n",
       "max    16.587707  18.232503  16.257095  16.098789  15.139805  12.745849"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_cv = pd.DataFrame(data=error_sum,columns = lambda_candi)\n",
    "cv_sta = error_cv.describe()\n",
    "mean = cv_sta.loc['mean']\n",
    "std = cv_sta.loc['std']\n",
    "print('mean:\\n',mean)\n",
    "print('standart deviation:\\n',std)\n",
    "cv_sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Regularizion Term $\\lambda ||w||$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianglongtan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal loss:  0.3076377121645377\n"
     ]
    }
   ],
   "source": [
    "def gradient_L1reg(y,x,w,lmd):\n",
    "    #loss = logitloss(sigmoid(w,x),y).reshape(y.shape[0],)\n",
    "    reg = lmd*np.ones((1501,1))\n",
    "    reg[0] = 0 # not regularize the bias term\n",
    "    return np.add(reg,np.dot(np.diag(np.subtract(sigmoid(w,x).reshape(y.shape[0],),y)),x)).mean(axis=0).reshape((1501,1))\n",
    "\n",
    "def gradientDescent_L1reg(y,w,x,eta,lmd):\n",
    "    for i in range(1000):\n",
    "        lossPre = logitloss(sigmoid(w,x),y).sum()\n",
    "        w = np.subtract(w,np.multiply(eta,gradient_L1reg(y,x,w,lmd)))\n",
    "        lossPost = logitloss(sigmoid(w,x),y).sum()\n",
    "        if (lossPre-lossPost)/lossPre<0.01:\n",
    "            print('optimal loss: ',lossPost)\n",
    "            break\n",
    "    return w\n",
    "\n",
    "w_opt_l1 = gradientDescent_L1reg(train_y, w_init, train_x, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient descent with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianglongtan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal loss:  12.44875083638241\n"
     ]
    }
   ],
   "source": [
    "def gradient_SGD(y,x,w,lmd):\n",
    "    #loss = logitloss(sigmoid(w,x),y).reshape(y.shape[0],)\n",
    "    reg = lmd*w\n",
    "    reg[0] = 0 # not regularize the bias term\n",
    "    gradient = ((sigmoid_SGD(w,x)-y)*x).reshape((((sigmoid_SGD(w,x)-y)*x).shape[0],1))+reg\n",
    "    return gradient\n",
    "\n",
    "def sigmoid_SGD(w,x):\n",
    "    result = np.exp(np.dot(x,w))/(1+np.exp(np.dot(x,w)))\n",
    "    return result\n",
    "\n",
    "def SGD_L2(y,w,x,eta,lmd):\n",
    "    row = np.arange(x.shape[0])\n",
    "    np.random.shuffle(row)\n",
    "    for i in range(1000):\n",
    "        lossPre = logitloss(sigmoid(w,x),y).sum()\n",
    "        for j in range(len(row)):\n",
    "            w = w - eta*gradient_SGD(y[j],x[j],w,lmd)\n",
    "        lossPost = logitloss(sigmoid(w,x),y).sum()\n",
    "        if (lossPre-lossPost)/lossPre<0.01:\n",
    "            print('optimal loss: ',lossPost)\n",
    "            break\n",
    "    return w\n",
    "w_opt_SGD=SGD_L2(train_y, w_init, train_x, 0.01, 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Linear Least Square  \n",
    "We may face the problem of heteroskedasticity in regression, so we implement weighted least square.  \n",
    "\n",
    "The mathematical form of weighted linear least square is minimizing $\\sum_{i=1}^nw_i(y_i-x_i\\beta)^2$.   \n",
    "\n",
    "In matrix:\n",
    "$$min\\  (Y-X\\beta)^TW(Y-X\\beta)$$  \n",
    "\n",
    "Here, we assume the weight matrix is $W^{-1}$, $W$ is diagonal matrix of error.  \n",
    "\n",
    "Hence, beta would be:\n",
    "$$\\beta^* = (X^TWX)^{-1}X^TWY$$\n",
    "\n",
    "Add a L2 regularization term:\n",
    "$$min\\  (Y-X\\beta)^TW(Y-X\\beta) + \\frac{\\lambda}{2} ||\\beta||^2$$  \n",
    "$$\\beta^* = (X^TWX+\\lambda I)^{-1}X^TWY$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent: 1.3622729778289795\n",
      "time spent: 1.5403039455413818\n",
      "[[ 25.8992379 ]\n",
      " [184.39661351]\n",
      " [  2.49887839]\n",
      " [  4.41977486]\n",
      " [  8.20673336]\n",
      " [ 12.75732468]\n",
      " [  3.06004279]\n",
      " [  2.11111552]\n",
      " [  6.37858705]\n",
      " [  1.84239537]]\n",
      "[[ 25.8992379   21.78042674]\n",
      " [184.39661351  53.73338445]\n",
      " [  2.49887839   2.60874292]\n",
      " [  4.41977486   3.94462904]\n",
      " [  8.20673336   8.1422325 ]\n",
      " [ 12.75732468  10.77467758]\n",
      " [  3.06004279   2.94131683]\n",
      " [  2.11111552   1.84457153]\n",
      " [  6.37858705   6.77942798]\n",
      " [  1.84239537   2.33273667]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6ced19438b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mw_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWLS_L2reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6ced19438b27>\u001b[0m in \u001b[0;36mWLS_L2reg\u001b[0;34m(X, y, lmd)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_ols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mb_wls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_wls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def WLS(X,y):\n",
    "    b_ols = (np.dot(X.T,X).T).dot(np.dot(X.T,y))\n",
    "    error = np.square(y - np.dot(X,b_ols))\n",
    "    weight = np.diag(np.divide(1,error).reshape(error.shape[0]))\n",
    "    b_wls = np.linalg.inv(np.dot(X.T,np.dot(weight,X))).dot(np.dot(X.T,np.dot(weight,y)))\n",
    "    return np.array(b_wls)\n",
    "start = time.time()\n",
    "b_wls = WLS(train_x,train_y)\n",
    "end = time.time()\n",
    "print('time spent:',end-start)\n",
    "\n",
    "def WLS_L2reg(X,y,lmd):\n",
    "    b_ols = (np.dot(X.T,X).T).dot(np.dot(X.T,y))\n",
    "    error = np.square(y - np.dot(X,b_ols))\n",
    "    weight = np.diag(np.divide(1,error).reshape(error.shape[0]))\n",
    "    b_wls = np.linalg.inv(np.add(np.dot(X.T,np.dot(weight,X)),lmd*np.ones((X.shape[1],X.shape[1])))).dot(np.dot(X.T,np.dot(weight,y)))\n",
    "    return np.array(b_wls)\n",
    "\n",
    "start = time.time()\n",
    "b_wls = WLS_L2reg(train_x,train_y,0.1)\n",
    "end = time.time()\n",
    "print('time spent:',end-start)\n",
    "\n",
    "def sqloss(y_,y):\n",
    "    return np.mean(np.square(np.subtract(y_,y)))\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "lambda_candi = [0,0.01,0.1,1,10,100]\n",
    "error_sum = []\n",
    "i = 1\n",
    "for lmd in lambda_candi:\n",
    "    error = []\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        w_init = np.random.rand(1501,1)\n",
    "        X_train, y_train = train_x[train_index], train_y[train_index]\n",
    "        X_test, y_test = train_x[test_index], train_y[test_index]\n",
    "        w_opt = WLS_L2reg(X_train,y_train,lmd)\n",
    "        loss = sqloss(np.dot(X_test,w_opt), y_test)\n",
    "        error.append(loss)\n",
    "    error = np.array(error).reshape(10,1)\n",
    "    if i==1:\n",
    "        error_sum = error\n",
    "    else:\n",
    "        error_sum = np.hstack((error_sum,error))\n",
    "    print(error_sum)\n",
    "    i += 1\n",
    "print('Done!')   \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test.sms.values\n",
    "test_y = test.label.values\n",
    "test_token = vectorizer.transform(test_x)\n",
    "test_token_idf = transformer.transform(test_token)\n",
    "df_test_token = pd.DataFrame(data=test_token_idf.toarray(),columns=colname)\n",
    "standard_df_test_token = standardScaler.transform(df_test_token)\n",
    "df_test_token_pca=pca.transform(standard_df_test_token)\n",
    "test_x = np.hstack((np.ones((test_y.shape[0],1)),df_test_token_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
